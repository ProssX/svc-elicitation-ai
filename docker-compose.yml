services:
  # Ollama service (Local LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  elicitation-ai:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: svc-elicitation-ai
    ports:
      - "8001:8001"
    environment:
      # Application
      - APP_ENV=${APP_ENV:-development}
      - APP_PORT=8001
      - APP_HOST=0.0.0.0
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
      # CORS
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:5173}
      
      # Backend PHP Service
      - BACKEND_PHP_URL=${BACKEND_PHP_URL:-http://host.docker.internal:8000/api/v1}
      
      # Model Provider (local or openai)
      - MODEL_PROVIDER=${MODEL_PROVIDER:-local}
      
      # OpenAI (only if MODEL_PROVIDER=openai)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o}
      
      # Ollama (only if MODEL_PROVIDER=local)
      # Use the service name as hostname
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:3b
      
      # Interview Configuration
      - MIN_QUESTIONS=${MIN_QUESTIONS:-7}
      - MAX_QUESTIONS=${MAX_QUESTIONS:-20}
    
    volumes:
      # Mount data directory for mock users
      - ./data:/app/data:ro
    
    depends_on:
      ollama:
        condition: service_healthy
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/api/v1/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# Volumes for persistent data
volumes:
  ollama-data:
    driver: local

# Networks configuration (optional)
# Uncomment if you need to connect this service with other Docker services
# networks:
#   tesis-network:
#     driver: bridge

# Usage:
# 1. Start services:
#    docker-compose up -d
#
# 2. Pull Ollama model (first time only):
#    docker exec -it ollama-service ollama pull llama3.2:3b
#
# 3. View logs:
#    docker-compose logs -f elicitation-ai
#    docker-compose logs -f ollama
#
# 4. Stop services:
#    docker-compose down
#
# 5. Stop and remove volumes (WARNING: deletes Ollama models):
#    docker-compose down -v
