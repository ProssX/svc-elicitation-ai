# ============================================
# Elicitation AI Service - Environment Config
# ============================================
# Copy this file to .env and fill in your values
# Command: cp env.example .env

# --------------------------------------------
# Application Configuration
# --------------------------------------------
APP_ENV=development
APP_PORT=8001
APP_HOST=0.0.0.0
LOG_LEVEL=INFO

# --------------------------------------------
# CORS Configuration
# --------------------------------------------
FRONTEND_URL=http://localhost:5173

# --------------------------------------------
# Backend PHP Service
# --------------------------------------------
BACKEND_PHP_URL=http://localhost:8000/api/v1

# --------------------------------------------
# Model Provider
# --------------------------------------------
# Options: "local" (Ollama) or "openai" (OpenAI API)
MODEL_PROVIDER=local

# --------------------------------------------
# OpenAI Configuration (only if MODEL_PROVIDER=openai)
# --------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# --------------------------------------------
# Ollama Configuration (only if MODEL_PROVIDER=local)
# --------------------------------------------
# Ensure Ollama is running: ollama serve
# Pull model: ollama pull llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# --------------------------------------------
# Interview Configuration
# --------------------------------------------
# Minimum questions before allowing interview to finish
MIN_QUESTIONS=7

# Maximum questions before forcing interview to finish
MAX_QUESTIONS=20

# Completeness threshold (0.0 - 1.0)
DEFAULT_COMPLETENESS_THRESHOLD=0.8

# --------------------------------------------
# Notes:
# --------------------------------------------
# 1. For LOCAL model (Ollama):
#    - Set MODEL_PROVIDER=local
#    - Make sure Ollama is running: ollama serve
#    - Download model: ollama pull llama3.2:3b
#
# 2. For OpenAI:
#    - Set MODEL_PROVIDER=openai
#    - Set your OPENAI_API_KEY
#    - Choose model: gpt-4o, gpt-4o-mini, gpt-4-turbo, etc.
#
# 3. For Docker:
#    - Use host.docker.internal instead of localhost for Ollama
#    - Example: OLLAMA_BASE_URL=http://host.docker.internal:11434

