# ============================================
# Elicitation AI Service - Environment Config
# ============================================
# Copy this file to .env and fill in your values
# Command: cp env.example .env

# --------------------------------------------
# Application Configuration
# --------------------------------------------
APP_ENV=development
APP_PORT=8001
APP_HOST=0.0.0.0
LOG_LEVEL=INFO

# --------------------------------------------
# Database Configuration
# --------------------------------------------
# PostgreSQL connection URL (asyncpg driver required)
# Format: postgresql+asyncpg://user:password@host:port/database
# For local development:
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/elicitation_ai
# For Docker Compose:
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/elicitation_ai

# Connection pool settings
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=10
DB_POOL_TIMEOUT=30
DB_POOL_RECYCLE=3600

# --------------------------------------------
# CORS Configuration
# --------------------------------------------
FRONTEND_URL=http://localhost:5173

# --------------------------------------------
# Backend PHP Service
# --------------------------------------------
BACKEND_PHP_URL=http://localhost:8000/api/v1

# --------------------------------------------
# Authentication Service
# --------------------------------------------
# URL of the Auth Service (svc-users-python)
# This service provides JWT token validation via JWKS endpoint
AUTH_SERVICE_URL=http://localhost:8000

# JWT token issuer (must match tokens from Auth Service)
JWT_ISSUER=https://api.example.com

# JWT token audience (must match tokens from Auth Service)
JWT_AUDIENCE=https://api.example.com

# JWKS cache time-to-live in seconds (default: 3600 = 1 hour)
JWKS_CACHE_TTL=3600

# --------------------------------------------
# Model Provider
# --------------------------------------------
# Options: "local" (Ollama) or "openai" (OpenAI API)
MODEL_PROVIDER=local

# --------------------------------------------
# OpenAI Configuration (only if MODEL_PROVIDER=openai)
# --------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# --------------------------------------------
# Ollama Configuration (only if MODEL_PROVIDER=local)
# --------------------------------------------
# Ensure Ollama is running: ollama serve
# Pull model: ollama pull llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# --------------------------------------------
# Interview Configuration
# --------------------------------------------
# Minimum questions before allowing interview to finish
MIN_QUESTIONS=7

# Maximum questions before forcing interview to finish
MAX_QUESTIONS=20

# Completeness threshold (0.0 - 1.0)
DEFAULT_COMPLETENESS_THRESHOLD=0.8

# Dynamic Interview Completion
# Enable dynamic completion (agent decides when to finish based on content quality)
ENABLE_DYNAMIC_COMPLETION=false

# Safety limit to prevent infinite loops (only used when dynamic completion is enabled)
MAX_QUESTIONS_SAFETY_LIMIT=50

# --------------------------------------------
# Context Enrichment Feature Flags
# --------------------------------------------
# Enable context enrichment (employee, organization, processes)
ENABLE_CONTEXT_ENRICHMENT=true

# Enable process matching during interviews
ENABLE_PROCESS_MATCHING=true

# --------------------------------------------
# Natural Interview Experience Feature Flags
# --------------------------------------------
# Enable improved natural language prompts (default: false)
# When enabled: Uses accessible, natural language without technical jargon
# When disabled: Uses original technical prompts with "Analista Senior" terminology
ENABLE_IMPROVED_PROMPTS=false

# Enable semantic process detection (default: false)
# When enabled: Uses AI-based semantic analysis to detect process mentions
# When disabled: Uses simple keyword-based detection
ENABLE_SEMANTIC_PROCESS_DETECTION=false

# Context cache time-to-live in seconds (default: 300 = 5 minutes)
CONTEXT_CACHE_TTL=300

# Process matching timeout in seconds (default: 3)
PROCESS_MATCHING_TIMEOUT=3

# Maximum number of processes to include in context (default: 20)
MAX_PROCESSES_IN_CONTEXT=20

# --------------------------------------------
# Process Detection Configuration (Rigorous Detection)
# --------------------------------------------
# Timeout for process detection in seconds (default: 3.0)
# Increased for rigorous detection to allow comprehensive semantic analysis
PROCESS_DETECTION_TIMEOUT=3.0

# Enable automatic retry on detection failures (default: true)
# If detection fails or times out, retry once before falling back
ENABLE_DETECTION_RETRY=true

# Minimum confidence score to consider a process detected (default: 0.6)
# Range: 0.0 to 1.0 (higher = more strict)
PROCESS_DETECTION_CONFIDENCE_THRESHOLD=0.6

# --------------------------------------------
# Notes:
# --------------------------------------------
# 1. For LOCAL model (Ollama):
#    - Set MODEL_PROVIDER=local
#    - Make sure Ollama is running: ollama serve
#    - Download model: ollama pull llama3.2:3b
#
# 2. For OpenAI:
#    - Set MODEL_PROVIDER=openai
#    - Set your OPENAI_API_KEY
#    - Choose model: gpt-4o, gpt-4o-mini, gpt-4-turbo, etc.
#
# 3. For Docker:
#    - Use host.docker.internal instead of localhost for Ollama
#    - Example: OLLAMA_BASE_URL=http://host.docker.internal:11434


