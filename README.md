# ğŸ¤– Elicitation AI Service

Microservicio de IA para elicitaciÃ³n de requerimientos mediante entrevistas conversacionales inteligentes.

## ğŸ“‹ **Â¿QuÃ© es este servicio?**

Este microservicio utiliza **LLMs (Large Language Models)** para conducir entrevistas automatizadas con usuarios de negocio, extrayendo informaciÃ³n relevante sobre procesos, necesidades y requerimientos de manera natural y conversacional.

**CaracterÃ­sticas principales:**
- âœ… Entrevistas adaptativas en **3 idiomas** (EspaÃ±ol, InglÃ©s, PortuguÃ©s)
- âœ… Soporte para modelos **locales (Ollama)** y **cloud (OpenAI)**
- âœ… API REST con FastAPI
- âœ… Arquitectura stateless y escalable
- âœ… Dockerizado para fÃ¡cil despliegue
- âœ… Multi-tenant con contexto de usuario/organizaciÃ³n

---

## ğŸ—ï¸ **Arquitectura**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (React)                        â”‚
â”‚  - Chat UI                                      â”‚
â”‚  - Multi-language selector                     â”‚
â”‚  - localStorage persistence                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTP/REST
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Elicitation AI Service (FastAPI)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Routers                                â”‚   â”‚
â”‚  â”‚  - /api/v1/health                       â”‚   â”‚
â”‚  â”‚  - /api/v1/interviews/start             â”‚   â”‚
â”‚  â”‚  - /api/v1/interviews/continue          â”‚   â”‚
â”‚  â”‚  - /api/v1/interviews/export            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Agent Service (Strands SDK)            â”‚   â”‚
â”‚  â”‚  - Interview management                 â”‚   â”‚
â”‚  â”‚  - Conversation context analysis        â”‚   â”‚
â”‚  â”‚  - Multi-language support               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Model Factory                          â”‚   â”‚
â”‚  â”‚  - OllamaModel (local)                  â”‚   â”‚
â”‚  â”‚  - OpenAIModel (cloud)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama         â”‚  â”‚ OpenAI API       â”‚
â”‚ (Local)        â”‚  â”‚ (Cloud)          â”‚
â”‚ - llama3.2:3b  â”‚  â”‚ - gpt-4o         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ **Setup RÃ¡pido**

### **Prerrequisitos**
- Python 3.11+
- pip
- **OpciÃ³n A**: Ollama instalado (para modelo local)
- **OpciÃ³n B**: API Key de OpenAI (para modelo cloud)

### **1. Clonar el repositorio**
```bash
git clone <repo-url>
cd svc-elicitation-ai
```

### **2. Crear entorno virtual**
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

### **3. Instalar dependencias**
```bash
pip install -r requirements.txt
```

### **4. Configurar variables de entorno**
```bash
# Copiar archivo de ejemplo
cp env.example .env

# Editar .env con tus valores
# Ver secciÃ³n "ConfiguraciÃ³n" mÃ¡s abajo
```

### **5. Ejecutar el servicio**
```bash
# Con hot-reload (desarrollo)
uvicorn app.main:app --reload --port 8001

# ProducciÃ³n
uvicorn app.main:app --host 0.0.0.0 --port 8001
```

### **6. Verificar que funciona**
```bash
# Health check
curl http://localhost:8001/api/v1/health

# DocumentaciÃ³n interactiva
# Abrir en navegador: http://localhost:8001/docs
```

---

## âš™ï¸ **ConfiguraciÃ³n**

### **OpciÃ³n A: Modelo Local con Ollama** âš¡ (Recomendado para desarrollo)

**Ventajas:**
- âœ… Gratis, sin costos
- âœ… Privacidad total (los datos no salen de tu mÃ¡quina)
- âœ… Sin lÃ­mites de requests

**Desventajas:**
- âŒ Requiere GPU para buen rendimiento
- âŒ Calidad menor que GPT-4o

#### **Paso 1: Instalar Ollama**
```bash
# Windows/Mac/Linux
# Descargar de: https://ollama.com/download
```

#### **Paso 2: Descargar modelo**
```bash
# Modelo recomendado (3B parÃ¡metros, ~2GB)
ollama pull llama3.2:3b

# O modelo mÃ¡s grande (mejor calidad, requiere mÃ¡s RAM)
ollama pull llama3.1:8b
```

#### **Paso 3: Iniciar Ollama**
```bash
ollama serve
```

#### **Paso 4: Configurar .env**
```bash
MODEL_PROVIDER=local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

---

### **OpciÃ³n B: Modelo Cloud con OpenAI** ğŸš€ (Recomendado para producciÃ³n)

**Ventajas:**
- âœ… MÃ¡xima calidad (GPT-4o es muy superior)
- âœ… No requiere GPU local
- âœ… Respuestas mÃ¡s rÃ¡pidas y contextuales

**Desventajas:**
- âŒ Costo por request (~$0.01 por entrevista)
- âŒ Requiere conexiÃ³n a internet
- âŒ Los datos se envÃ­an a OpenAI

#### **Paso 1: Obtener API Key**
1. Crear cuenta en: https://platform.openai.com/
2. Ir a: https://platform.openai.com/api-keys
3. Crear nueva API key
4. Copiar la key (guÃ¡rdala, no se vuelve a mostrar)

#### **Paso 2: Configurar .env**
```bash
MODEL_PROVIDER=openai
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXXXXXXXXXX
OPENAI_MODEL=gpt-4o
```

#### **Paso 3: Ejecutar y probar**
```bash
# Si estÃ¡s en Windows y el .env no se carga automÃ¡ticamente:
# PowerShell:
$env:OPENAI_API_KEY="sk-proj-XXXXX"
uvicorn app.main:app --reload --port 8001

# CMD:
set OPENAI_API_KEY=sk-proj-XXXXX
uvicorn app.main:app --reload --port 8001

# Linux/Mac:
export OPENAI_API_KEY="sk-proj-XXXXX"
uvicorn app.main:app --reload --port 8001
```

---

## ğŸ³ **Docker**

### **OpciÃ³n 1: docker-compose (Recomendado)**

#### **Con Ollama (Local)**
```bash
# 1. Asegurarse de que Ollama estÃ© corriendo en host
ollama serve

# 2. Configurar .env
MODEL_PROVIDER=local

# 3. Levantar servicio
docker-compose up -d

# 4. Ver logs
docker-compose logs -f elicitation-ai
```

#### **Con OpenAI (Cloud)**
```bash
# 1. Configurar .env
MODEL_PROVIDER=openai
OPENAI_API_KEY=sk-proj-XXXXXXXX

# 2. Levantar servicio
docker-compose up -d

# 3. Ver logs
docker-compose logs -f elicitation-ai
```

### **OpciÃ³n 2: Docker standalone**
```bash
# Build
docker build -t svc-elicitation-ai:latest .

# Run con Ollama
docker run -d \
  -p 8001:8001 \
  -e MODEL_PROVIDER=local \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name elicitation-ai \
  svc-elicitation-ai:latest

# Run con OpenAI
docker run -d \
  -p 8001:8001 \
  -e MODEL_PROVIDER=openai \
  -e OPENAI_API_KEY=sk-proj-XXXXXXXX \
  --name elicitation-ai \
  svc-elicitation-ai:latest
```

---

## ğŸ“¡ **API Endpoints**

### **1. Health Check**
```bash
GET /api/v1/health

# Respuesta:
{
  "status": "healthy",
  "model_provider": "openai",
  "model_id": "gpt-4o"
}
```

### **2. Iniciar Entrevista**
```bash
POST /api/v1/interviews/start
Content-Type: application/json

{
  "user_id": "user-123",
  "organization_id": "org-456",
  "role_id": 1,
  "language": "es"
}

# Respuesta:
{
  "status": "success",
  "data": {
    "session_id": "uuid",
    "question": "Hola Juan! Â¿CÃ³mo andÃ¡s? Soy el Agente ProssX...",
    "question_number": 1,
    "is_final": false,
    "context": {
      "processes_identified": [],
      "topics_discussed": [],
      "completeness": 0.0
    }
  }
}
```

### **3. Continuar Entrevista**
```bash
POST /api/v1/interviews/continue
Content-Type: application/json

{
  "session_id": "uuid",
  "user_response": "Soy analista de compras...",
  "conversation_history": [
    {"role": "assistant", "content": "..."},
    {"role": "user", "content": "..."}
  ],
  "language": "es"
}
```

### **4. Exportar Entrevista**
```bash
POST /api/v1/interviews/export
Content-Type: application/json

{
  "session_id": "uuid"
}

# Retorna:
# - ConversaciÃ³n completa
# - Metadata del usuario
# - MÃ©tricas (duraciÃ³n, completeness, etc.)
# - NO incluye anÃ¡lisis de procesos (eso es responsabilidad de otro servicio)
```

---

## ğŸ” **Troubleshooting**

### **Problema: "Module 'openai' not found"**
```bash
# SoluciÃ³n: Instalar openai
pip install openai>=1.0.0
```

### **Problema: "OpenAI API key not set"**
```bash
# SoluciÃ³n 1: Verificar .env
cat .env | grep OPENAI_API_KEY

# SoluciÃ³n 2: Exportar manualmente
export OPENAI_API_KEY="sk-proj-XXXXX"
```

### **Problema: "Cannot connect to Ollama"**
```bash
# SoluciÃ³n 1: Verificar que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# SoluciÃ³n 2: Iniciar Ollama
ollama serve

# SoluciÃ³n 3: Verificar modelo descargado
ollama list
```

### **Problema: Respuestas lentas con Ollama**
```bash
# Causa: Modelo muy grande para tu hardware
# SoluciÃ³n: Usar modelo mÃ¡s pequeÃ±o
ollama pull llama3.2:3b  # En vez de 8b o 70b
```

### **Problema: "Interview completed" pero sigue preguntando**
```bash
# Causa: Bug conocido (ya resuelto en Ãºltima versiÃ³n)
# SoluciÃ³n: Hacer git pull para obtener el fix
git pull origin main
```

---

## ğŸ“ **Estructura del Proyecto**

```
svc-elicitation-ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Entry point FastAPI
â”‚   â”œâ”€â”€ config.py               # Settings con Pydantic
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ interview.py        # Pydantic models
â”‚   â”‚   â””â”€â”€ responses.py        # API response schemas
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ health.py           # Health check endpoint
â”‚   â”‚   â””â”€â”€ interviews.py       # Interview endpoints
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ agent_service.py    # Core interview agent
â”‚       â”œâ”€â”€ context_service.py  # User context management
â”‚       â””â”€â”€ model_factory.py    # LLM factory (Ollama/OpenAI)
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ system_prompts.py       # System prompts (ES/EN/PT)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mock_users.json         # Mock user data (MVP)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Docker image
â”œâ”€â”€ docker-compose.yml          # Docker Compose config
â”œâ”€â”€ env.example                 # Environment variables template
â””â”€â”€ README.md                   # Este archivo
```

---

## ğŸ› ï¸ **Variables de Entorno (Completas)**

| Variable | DescripciÃ³n | Default | Requerido |
|----------|-------------|---------|-----------|
| `APP_ENV` | Entorno (development/production) | `development` | No |
| `APP_PORT` | Puerto del servicio | `8001` | No |
| `APP_HOST` | Host bind | `0.0.0.0` | No |
| `LOG_LEVEL` | Nivel de logs | `INFO` | No |
| `FRONTEND_URL` | URL del frontend (CORS) | `http://localhost:5173` | No |
| `BACKEND_PHP_URL` | URL del backend PHP | `http://localhost:8000/api/v1` | No |
| **`MODEL_PROVIDER`** | **Proveedor: `local` o `openai`** | `local` | **SÃ­** |
| `OPENAI_API_KEY` | API Key de OpenAI | - | Solo si `openai` |
| `OPENAI_MODEL` | Modelo de OpenAI | `gpt-4o` | No |
| `OLLAMA_BASE_URL` | URL de Ollama | `http://localhost:11434` | Solo si `local` |
| `OLLAMA_MODEL` | Modelo de Ollama | `llama3.2:3b` | No |
| `MIN_QUESTIONS` | MÃ­nimo de preguntas | `7` | No |
| `MAX_QUESTIONS` | MÃ¡ximo de preguntas | `20` | No |

---

## ğŸ§ª **Testing**

```bash
# Test rÃ¡pido (sin LLM)
curl -X POST http://localhost:8001/api/v1/interviews/test \
  -H "Content-Type: application/json" \
  -d '{"user_id":"test","organization_id":"org","role_id":1,"language":"es"}'

# Test completo (con LLM)
curl -X POST http://localhost:8001/api/v1/interviews/start \
  -H "Content-Type: application/json" \
  -d '{"user_id":"user-123","organization_id":"org-456","role_id":1,"language":"es"}'
```

---

## ğŸ“Š **Stack TecnolÃ³gico**

- **Framework**: FastAPI 0.115.6
- **AI SDK**: Strands Agents >=1.0.0
- **LLM Local**: Ollama + Llama 3.2
- **LLM Cloud**: OpenAI GPT-4o
- **Language**: Python 3.11+
- **Config**: Pydantic Settings
- **HTTP Client**: httpx, requests
- **Containerization**: Docker + Docker Compose

---

## ğŸ¤ **IntegraciÃ³n con otros servicios**

Este microservicio es parte de un sistema mayor:

1. **Frontend (React)**: `web-frontend-react/`
   - Consume este servicio para el chat
   - Maneja la UI y persistencia en localStorage

2. **Backend PHP**: `svc-organizations-php/`
   - Provee contexto de usuarios/organizaciones
   - PersistirÃ¡ entrevistas completadas (futuro)

3. **AnÃ¡lisis de Procesos** (Futuro):
   - ConsumirÃ¡ el endpoint `/export`
   - ExtraerÃ¡ procesos estructurados
   - GenerarÃ¡ diagramas BPMN

---

## ğŸ“ **PrÃ³ximos Pasos**

- [ ] Implementar persistencia en PostgreSQL (actualmente solo localStorage)
- [ ] Agregar anÃ¡lisis de sentimientos
- [ ] Spell checking en espaÃ±ol
- [ ] Soporte para mÃ¡s idiomas (FR, IT, DE)
- [ ] MÃ©tricas con Prometheus
- [ ] Tests automatizados (pytest)
- [ ] CI/CD pipeline

---

## ğŸ“„ **Licencia**

Este proyecto es parte de una tesis de grado.

---

## ğŸ‘¨â€ğŸ’» **Autor**

Desarrollado como parte del proyecto de tesis "Sistema de ElicitaciÃ³n de Requerimientos con IA".

---

## ğŸ†˜ **Soporte**

Si encontrÃ¡s algÃºn problema:
1. VerificÃ¡ la secciÃ³n **Troubleshooting**
2. RevisÃ¡ los logs: `docker-compose logs -f elicitation-ai`
3. ConsultÃ¡ la documentaciÃ³n interactiva: http://localhost:8001/docs
4. RevisÃ¡ las issues en el repositorio

---

**Â¡Listo para empezar! ğŸš€**
