# ğŸ¤– Elicitation AI Service

Microservicio de IA para elicitaciÃ³n de requerimientos mediante entrevistas conversacionales inteligentes.

## ğŸ“‹ **Â¿QuÃ© es este servicio?**

Este microservicio utiliza **LLMs (Large Language Models)** para conducir entrevistas automatizadas con usuarios de negocio, extrayendo informaciÃ³n relevante sobre procesos, necesidades y requerimientos de manera natural y conversacional.

**CaracterÃ­sticas principales:**
- âœ… Entrevistas adaptativas en **3 idiomas** (EspaÃ±ol, InglÃ©s, PortuguÃ©s)
- âœ… Soporte para modelos **locales (Ollama)** y **cloud (OpenAI)**
- âœ… API REST con FastAPI
- âœ… Arquitectura stateless y escalable
- âœ… Dockerizado para fÃ¡cil despliegue
- âœ… Multi-tenant con contexto de usuario/organizaciÃ³n

---

## ğŸ—ï¸ **Arquitectura**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (React)                        â”‚
â”‚  - Chat UI                                      â”‚
â”‚  - Multi-language selector                     â”‚
â”‚  - localStorage persistence                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTP/REST
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Elicitation AI Service (FastAPI)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Routers                                â”‚   â”‚
â”‚  â”‚  - /api/v1/health                       â”‚   â”‚
â”‚  â”‚  - /api/v1/interviews/start             â”‚   â”‚
â”‚  â”‚  - /api/v1/interviews/continue          â”‚   â”‚
â”‚  â”‚  - /api/v1/interviews/export            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Agent Service (Strands SDK)            â”‚   â”‚
â”‚  â”‚  - Interview management                 â”‚   â”‚
â”‚  â”‚  - Conversation context analysis        â”‚   â”‚
â”‚  â”‚  - Multi-language support               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Model Factory                          â”‚   â”‚
â”‚  â”‚  - OllamaModel (local)                  â”‚   â”‚
â”‚  â”‚  - OpenAIModel (cloud)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama         â”‚  â”‚ OpenAI API       â”‚
â”‚ (Local)        â”‚  â”‚ (Cloud)          â”‚
â”‚ - llama3.2:3b  â”‚  â”‚ - gpt-4o         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ **Inicio RÃ¡pido con Docker** (Recomendado)

### **Prerrequisitos**
- Docker Desktop instalado y corriendo
- Git

**âœ¨ Ventaja de usar Docker:**
- âœ… **NO necesitÃ¡s instalar Ollama** en tu mÃ¡quina
- âœ… **NO necesitÃ¡s instalar Python** ni dependencias
- âœ… Todo corre aislado en contenedores
- âœ… FÃ¡cil de compartir con el equipo

### **Paso 1: Clonar el repositorio**
```bash
git clone <repo-url>
cd svc-elicitation-ai
```

### **Paso 2: Levantar los servicios**
```bash
# Levantar backend AI + Ollama
docker-compose up -d

# Ver los logs en tiempo real
docker-compose logs -f
```

### **Paso 3: Descargar el modelo de IA (solo primera vez)**
```bash
# Esto puede tardar ~2-3 minutos (descarga 2GB)
docker exec ollama-service ollama pull llama3.2:3b
```

### **Paso 4: Verificar que todo funciona**
```bash
# Windows PowerShell:
Invoke-RestMethod -Uri http://localhost:8001/api/v1/health -Method Get

# Linux/Mac/Git Bash:
curl http://localhost:8001/api/v1/health
```

**Respuesta esperada:**
```json
{
  "status": "success",
  "code": 200,
  "message": "Service is healthy",
  "data": {
    "service": "svc-elicitation-ai",
    "version": "1.0.0",
    "status": "healthy",
    "model_provider": "local",
    "model": "llama3.2:3b",
    "environment": "development"
  }
}
```

### **Paso 5: Probar una entrevista**
```bash
# Windows PowerShell:
$body = @{ language = 'es'; organization_id = '1'; role_id = '1' } | ConvertTo-Json
Invoke-RestMethod -Uri http://localhost:8001/api/v1/interviews/start -Method Post -Body $body -ContentType 'application/json'

# Linux/Mac/Git Bash:
curl -X POST http://localhost:8001/api/v1/interviews/start \
  -H "Content-Type: application/json" \
  -d '{"language":"es","organization_id":"1","role_id":"1"}'
```

**Â¡Listo! El servicio estÃ¡ corriendo en `http://localhost:8001` ğŸ‰**

**ğŸ“š Ver documentaciÃ³n interactiva:** http://localhost:8001/docs

---

## ğŸ› ï¸ **Setup Manual (Sin Docker)**

<details>
<summary>Click para expandir instrucciones de setup manual</summary>

**âš ï¸ Con setup manual SÃ necesitÃ¡s:**
- Instalar Python, pip, y todas las dependencias manualmente
- Instalar Ollama localmente (si usÃ¡s modelo local)
- Configurar variables de entorno

**RecomendaciÃ³n:** UsÃ¡ Docker si es tu primera vez.

---

### **Prerrequisitos**
- Python 3.11+
- pip
- **OpciÃ³n A**: Ollama instalado localmente (para modelo local)
- **OpciÃ³n B**: API Key de OpenAI (para modelo cloud)

### **1. Clonar el repositorio**
```bash
git clone <repo-url>
cd svc-elicitation-ai
```

### **2. Crear entorno virtual**
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

### **3. Instalar dependencias**
```bash
pip install -r requirements.txt
```

### **4. Configurar variables de entorno**
```bash
# Copiar archivo de ejemplo
cp env.example .env

# Editar .env con tus valores
# Ver secciÃ³n "ConfiguraciÃ³n" mÃ¡s abajo
```

### **5. Ejecutar el servicio**
```bash
# Con hot-reload (desarrollo)
uvicorn app.main:app --reload --port 8001

# ProducciÃ³n
uvicorn app.main:app --host 0.0.0.0 --port 8001
```

### **6. Verificar que funciona**
```bash
# Health check
curl http://localhost:8001/api/v1/health

# DocumentaciÃ³n interactiva
# Abrir en navegador: http://localhost:8001/docs
```

</details>

---

## âš™ï¸ **ConfiguraciÃ³n**

### **OpciÃ³n A: Modelo Local con Ollama** âš¡ (Recomendado para desarrollo)

**ğŸ“¦ Â¿CuÃ¡ndo necesito esto?**
- âœ… Si usÃ¡s **Docker**: Ya estÃ¡ incluido, solo seguÃ­ los pasos del "Inicio RÃ¡pido"
- âš ï¸ Si usÃ¡s **Setup Manual**: SeguÃ­ estos pasos para instalar Ollama localmente

**Ventajas:**
- âœ… Gratis, sin costos
- âœ… Privacidad total (los datos no salen de tu mÃ¡quina)
- âœ… Sin lÃ­mites de requests

**Desventajas:**
- âŒ Requiere GPU para buen rendimiento (CPU es muy lento)
- âŒ Calidad menor que GPT-4o

---

#### **Paso 1: Instalar Ollama (Solo si NO usÃ¡s Docker)**
```bash
# Windows/Mac/Linux
# Descargar de: https://ollama.com/download
```

#### **Paso 2: Descargar modelo**
```bash
# Modelo recomendado (3B parÃ¡metros, ~2GB)
ollama pull llama3.2:3b

# O modelo mÃ¡s grande (mejor calidad, requiere mÃ¡s RAM)
ollama pull llama3.1:8b
```

#### **Paso 3: Iniciar Ollama**
```bash
ollama serve
```

#### **Paso 4: Configurar .env**
```bash
MODEL_PROVIDER=local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

---

### **OpciÃ³n B: Modelo Cloud con OpenAI** ğŸš€ (Recomendado para producciÃ³n)

**Ventajas:**
- âœ… MÃ¡xima calidad (GPT-4o es muy superior)
- âœ… No requiere GPU local
- âœ… Respuestas mÃ¡s rÃ¡pidas y contextuales

**Desventajas:**
- âŒ Costo por request (~$0.01 por entrevista)
- âŒ Requiere conexiÃ³n a internet
- âŒ Los datos se envÃ­an a OpenAI

#### **Paso 1: Obtener API Key**
1. Crear cuenta en: https://platform.openai.com/
2. Ir a: https://platform.openai.com/api-keys
3. Crear nueva API key
4. Copiar la key (guÃ¡rdala, no se vuelve a mostrar)

#### **Paso 2: Configurar .env**
```bash
MODEL_PROVIDER=openai
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXXXXXXXXXX
OPENAI_MODEL=gpt-4o
```

#### **Paso 3: Ejecutar y probar**
```bash
# Si estÃ¡s en Windows y el .env no se carga automÃ¡ticamente:
# PowerShell:
$env:OPENAI_API_KEY="sk-proj-XXXXX"
uvicorn app.main:app --reload --port 8001

# CMD:
set OPENAI_API_KEY=sk-proj-XXXXX
uvicorn app.main:app --reload --port 8001

# Linux/Mac:
export OPENAI_API_KEY="sk-proj-XXXXX"
uvicorn app.main:app --reload --port 8001
```

---

## ğŸ³ **GestiÃ³n con Docker**

### **Comandos Ãštiles**

#### **Iniciar servicios**
```bash
# Iniciar en background
docker-compose up -d

# Iniciar y ver logs
docker-compose up

# Rebuild completo (si cambiaste cÃ³digo)
docker-compose up -d --build
```

#### **Ver logs**
```bash
# Ambos servicios
docker-compose logs -f

# Solo AI service
docker-compose logs -f elicitation-ai

# Solo Ollama
docker-compose logs -f ollama
```

#### **Detener servicios**
```bash
# Detener (mantiene los datos del modelo)
docker-compose down

# Detener y borrar TODO (incluyendo modelo descargado - 2GB)
docker-compose down -v
```

#### **Verificar estado**
```bash
# Ver contenedores corriendo
docker ps

# Ver salud de los servicios
docker inspect svc-elicitation-ai | grep -A 3 "Health"
docker inspect ollama-service | grep -A 3 "Health"
```

#### **Reiniciar un servicio**
```bash
# Reiniciar solo AI service
docker-compose restart elicitation-ai

# Reiniciar solo Ollama
docker-compose restart ollama
```

#### **Ejecutar comandos dentro de los contenedores**
```bash
# Ver modelos instalados en Ollama
docker exec ollama-service ollama list

# Descargar otro modelo
docker exec ollama-service ollama pull llama3.1:8b

# Verificar configuraciÃ³n del AI service
docker exec svc-elicitation-ai python -c "from app.config import Settings; s = Settings(); print(f'Provider: {s.model_provider}, Model: {s.ollama_model}')"
```

---

### **ğŸ”„ Cambiar de Ollama a OpenAI**

Si querÃ©s cambiar al modelo cloud de OpenAI:

```bash
# 1. Detener servicios
docker-compose down

# 2. Editar docker-compose.yml, cambiar estas lÃ­neas:
# - MODEL_PROVIDER=openai  # Cambiar de 'local' a 'openai'
# - OPENAI_API_KEY=sk-proj-TU_API_KEY_AQUI

# 3. Levantar solo el AI service (no necesitamos Ollama)
docker-compose up -d elicitation-ai

# 4. Verificar
curl http://localhost:8001/api/v1/health
```

---

## ğŸ“¡ **API Endpoints**

### **ğŸ“š DocumentaciÃ³n Interactiva**

FastAPI genera documentaciÃ³n automÃ¡tica en dos formatos:

#### **Swagger UI (Recomendado)**
```
ğŸŒ http://localhost:8001/docs
```
- âœ… Interfaz interactiva para probar endpoints
- âœ… Esquemas de request/response
- âœ… Ejecutar requests directamente desde el navegador
- âœ… Ver ejemplos y validaciones

#### **ReDoc (Alternativo)**
```
ğŸŒ http://localhost:8001/redoc
```
- âœ… DocumentaciÃ³n limpia y moderna
- âœ… BÃºsqueda avanzada
- âœ… Descarga de especificaciÃ³n OpenAPI

#### **OpenAPI JSON**
```
ğŸŒ http://localhost:8001/openapi.json
```
- Para integraciÃ³n con herramientas externas (Postman, Insomnia, etc.)

---

### **ğŸ”Œ Endpoints Disponibles**

#### **1. Health Check**
```bash
GET /api/v1/health

# Respuesta:
{
  "status": "success",
  "code": 200,
  "message": "Service is healthy",
  "data": {
    "service": "svc-elicitation-ai",
    "version": "1.0.0",
    "status": "healthy",
    "model_provider": "local",
    "model": "llama3.2:3b",
    "environment": "development"
  }
}
```

#### **2. Iniciar Entrevista**
**DescripciÃ³n:** Inicia una nueva sesiÃ³n de entrevista con el usuario.

**Request:**
```json
POST /api/v1/interviews/start
Content-Type: application/json

{
  "language": "es",              // "es" | "en" | "pt"
  "organization_id": "1",         // String (ID de organizaciÃ³n)
  "role_id": "1"                  // String (ID del rol del usuario)
}
```

**Ejemplo con PowerShell:**
```powershell
$body = @{ 
  language = 'es'
  organization_id = '1'
  role_id = '1' 
} | ConvertTo-Json

Invoke-RestMethod -Uri http://localhost:8001/api/v1/interviews/start `
  -Method Post -Body $body -ContentType 'application/json'
```

**Ejemplo con cURL:**
```bash
curl -X POST http://localhost:8001/api/v1/interviews/start \
  -H "Content-Type: application/json" \
  -d '{"language":"es","organization_id":"1","role_id":"1"}'
```

**Response:**
```json
{
  "status": "success",
  "code": 200,
  "message": "Interview started successfully",
  "data": {
    "session_id": "1add3c4a-8730-4140-888b-59ac47fcac43",
    "question": "Hola Juan, Â¿cÃ³mo vas? Me alegra tenerte aquÃ­ hoy...",
    "question_number": 1,
    "is_final": false,
    "context": {}
  },
  "errors": [],
  "meta": {
    "user_name": "Juan PÃ©rez",
    "organization": "ProssX Demo"
  }
}
```

#### **3. Continuar Entrevista**
**DescripciÃ³n:** EnvÃ­a la respuesta del usuario y recibe la siguiente pregunta.

**Request:**
```json
POST /api/v1/interviews/continue
Content-Type: application/json

{
  "session_id": "1add3c4a-8730-4140-888b-59ac47fcac43",
  "user_response": "Soy gerente de operaciones, coordino equipos...",
  "conversation_history": [],
  "language": "es"
}
```

**Ejemplo con PowerShell:**
```powershell
$body = @{ 
  session_id = '1add3c4a-8730-4140-888b-59ac47fcac43'
  user_response = 'Soy gerente de operaciones, coordino equipos y apruebo compras'
  conversation_history = @()
  language = 'es'
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri http://localhost:8001/api/v1/interviews/continue `
  -Method Post -Body $body -ContentType 'application/json'
```

**Ejemplo con cURL:**
```bash
curl -X POST http://localhost:8001/api/v1/interviews/continue \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "1add3c4a-8730-4140-888b-59ac47fcac43",
    "user_response": "Soy gerente de operaciones, coordino equipos",
    "conversation_history": [],
    "language": "es"
  }'
```

**Response:**
```json
{
  "status": "success",
  "code": 200,
  "message": "Question generated successfully",
  "data": {
    "question": "Vamos a profundizar un poco mÃ¡s. Â¿CuÃ¡l es el primer paso...",
    "question_number": 2,
    "is_final": false,
    "context": {},
    "corrected_response": "..."
  },
  "errors": [],
  "meta": {
    "session_id": "1add3c4a-8730-4140-888b-59ac47fcac43",
    "question_count": 2
  }
}
```

---

#### **4. Exportar Entrevista**
**DescripciÃ³n:** Exporta los datos completos de la entrevista para anÃ¡lisis posterior.

**Request:**
```json
POST /api/v1/interviews/export
Content-Type: application/json

{
  "session_id": "1add3c4a-8730-4140-888b-59ac47fcac43"
}
```

**Ejemplo con PowerShell:**
```powershell
$body = @{ session_id = '1add3c4a-8730-4140-888b-59ac47fcac43' } | ConvertTo-Json
Invoke-RestMethod -Uri http://localhost:8001/api/v1/interviews/export `
  -Method Post -Body $body -ContentType 'application/json'
```

**Ejemplo con cURL:**
```bash
curl -X POST http://localhost:8001/api/v1/interviews/export \
  -H "Content-Type: application/json" \
  -d '{"session_id":"1add3c4a-8730-4140-888b-59ac47fcac43"}'
```

**Response:**
```json
{
  "status": "success",
  "data": {
    "session_id": "uuid",
    "conversation": [
      {"role": "assistant", "content": "..."},
      {"role": "user", "content": "..."}
    ],
    "metadata": {
      "user_name": "Juan PÃ©rez",
      "organization": "ProssX Demo",
      "start_time": "2025-10-05T10:30:00Z",
      "duration_minutes": 15
    }
  }
}
```

**âš ï¸ Nota:** Este endpoint retorna **solo los datos en crudo**. El anÃ¡lisis de procesos (extracciÃ³n, estructuraciÃ³n, BPMN) es responsabilidad de otro microservicio que consumirÃ¡ estos datos.

---

## ğŸ” **Troubleshooting**

### **âŒ Error: "All connection attempts failed"**

**SÃ­ntoma:** El AI service no puede conectarse a Ollama

**Causas y Soluciones:**

1. **Ollama no estÃ¡ corriendo:**
```bash
# Verificar si Ollama estÃ¡ up
docker ps | grep ollama

# Si no aparece, reiniciar servicios
docker-compose restart
```

2. **Modelo no descargado:**
```bash
# Verificar modelos instalados
docker exec ollama-service ollama list

# Si estÃ¡ vacÃ­o, descargar el modelo
docker exec ollama-service ollama pull llama3.2:3b
```

3. **Variable de entorno incorrecta:**
```bash
# Verificar la URL dentro del contenedor
docker exec svc-elicitation-ai printenv | grep OLLAMA

# Debe mostrar: OLLAMA_BASE_URL=http://ollama:11434
# Si muestra localhost:11434, hay que editar docker-compose.yml
```

---

### **âŒ Error: "422 Unprocessable Entity" en `/start`**

**SÃ­ntoma:** Error de validaciÃ³n al iniciar entrevista

**Causa:** Los IDs deben ser strings, no nÃºmeros

**SoluciÃ³n:**
```bash
# âŒ Incorrecto:
{"organization_id": 1, "role_id": 1}

# âœ… Correcto:
{"organization_id": "1", "role_id": "1"}
```

---

### **âŒ Error: "Module 'openai' not found"**

**SÃ­ntoma:** ImportError al iniciar el servicio

**SoluciÃ³n:**
```bash
# Con Docker (rebuild)
docker-compose up -d --build

# Sin Docker
pip install openai>=1.0.0
```

---

### **âŒ Error: "OpenAI API key not set"**

**SÃ­ntoma:** AuthenticationError al usar OpenAI

**Soluciones:**

1. **Con Docker:**
```bash
# Editar docker-compose.yml, lÃ­nea ~42:
- OPENAI_API_KEY=sk-proj-TU_KEY_AQUI

# Recrear contenedor
docker-compose up -d --force-recreate elicitation-ai
```

2. **Sin Docker (Windows PowerShell):**
```powershell
$env:OPENAI_API_KEY="sk-proj-XXXXX"
uvicorn app.main:app --reload --port 8001
```

3. **Sin Docker (Linux/Mac):**
```bash
export OPENAI_API_KEY="sk-proj-XXXXX"
uvicorn app.main:app --reload --port 8001
```

---

### **âš ï¸ Problema: Respuestas MUY lentas con Ollama**

**Causa:** Modelo muy grande para tu CPU (sin GPU)

**Soluciones:**

1. **Usar modelo mÃ¡s pequeÃ±o:**
```bash
# Cambiar a modelo 3B (mÃ¡s rÃ¡pido, menor calidad)
docker exec ollama-service ollama pull llama3.2:3b

# Editar docker-compose.yml:
# - OLLAMA_MODEL=llama3.2:3b
```

2. **Cambiar a OpenAI (mÃ¡s rÃ¡pido pero de pago):**
```bash
# Ver secciÃ³n "Cambiar de Ollama a OpenAI"
```

---

### **âš ï¸ Problema: "Interview completed" pero sigue preguntando**

**Causa:** Bug en lÃ³gica de terminaciÃ³n (resuelto en v1.0.0)

**SoluciÃ³n:**
```bash
# Actualizar cÃ³digo
git pull origin main

# Rebuild servicios
docker-compose up -d --build
```

---

### **âš ï¸ Problema: Contenedor "unhealthy"**

**SÃ­ntoma:** `docker ps` muestra status "unhealthy"

**SoluciÃ³n:**
```bash
# Ver logs del contenedor
docker logs svc-elicitation-ai --tail 50

# Causas comunes:
# 1. Puerto 8001 ya en uso -> cambiar en docker-compose.yml
# 2. Ollama no disponible -> verificar ollama-service
# 3. Dependencias faltantes -> docker-compose up -d --build
```

---

### **âš ï¸ Problema: PC lenta / Docker consume mucha RAM**

**Causa:** Ollama + modelo cargado en memoria (~4-6GB RAM)

**Soluciones:**

1. **Detener Ollama cuando no lo uses:**
```bash
docker-compose stop ollama
```

2. **Usar OpenAI en lugar de Ollama local**

3. **Aumentar RAM de Docker Desktop:**
   - Docker Desktop â†’ Settings â†’ Resources â†’ Memory â†’ Aumentar a 6GB+

---

## ğŸ“ **Estructura del Proyecto**

```
svc-elicitation-ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Entry point FastAPI
â”‚   â”œâ”€â”€ config.py               # Settings con Pydantic
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ interview.py        # Pydantic models
â”‚   â”‚   â””â”€â”€ responses.py        # API response schemas
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ health.py           # Health check endpoint
â”‚   â”‚   â””â”€â”€ interviews.py       # Interview endpoints
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ agent_service.py    # Core interview agent
â”‚       â”œâ”€â”€ context_service.py  # User context management
â”‚       â””â”€â”€ model_factory.py    # LLM factory (Ollama/OpenAI)
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ system_prompts.py       # System prompts (ES/EN/PT)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mock_users.json         # Mock user data (MVP)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Docker image
â”œâ”€â”€ docker-compose.yml          # Docker Compose config
â”œâ”€â”€ env.example                 # Environment variables template
â””â”€â”€ README.md                   # Este archivo
```

---

## ğŸ› ï¸ **Variables de Entorno (Completas)**

| Variable | DescripciÃ³n | Default | Requerido |
|----------|-------------|---------|-----------|
| `APP_ENV` | Entorno (development/production) | `development` | No |
| `APP_PORT` | Puerto del servicio | `8001` | No |
| `APP_HOST` | Host bind | `0.0.0.0` | No |
| `LOG_LEVEL` | Nivel de logs | `INFO` | No |
| `FRONTEND_URL` | URL del frontend (CORS) | `http://localhost:5173` | No |
| `BACKEND_PHP_URL` | URL del backend PHP | `http://localhost:8000/api/v1` | No |
| **`MODEL_PROVIDER`** | **Proveedor: `local` o `openai`** | `local` | **SÃ­** |
| `OPENAI_API_KEY` | API Key de OpenAI | - | Solo si `openai` |
| `OPENAI_MODEL` | Modelo de OpenAI | `gpt-4o` | No |
| `OLLAMA_BASE_URL` | URL de Ollama | `http://ollama:11434` (Docker)<br>`http://localhost:11434` (Local) | Solo si `local` |
| `OLLAMA_MODEL` | Modelo de Ollama | `llama3.2:3b` | No |
| `MIN_QUESTIONS` | MÃ­nimo de preguntas | `7` | No |
| `MAX_QUESTIONS` | MÃ¡ximo de preguntas | `20` | No |

**âš ï¸ Nota importante sobre `OLLAMA_BASE_URL`:**
- En Docker: usar `http://ollama:11434` (nombre del servicio)
- Sin Docker: usar `http://localhost:11434` (localhost)
- Si ves error "All connection attempts failed", verificÃ¡ esta variable

---

## ğŸ§ª **Testing**

```bash
# Test rÃ¡pido (sin LLM)
curl -X POST http://localhost:8001/api/v1/interviews/test \
  -H "Content-Type: application/json" \
  -d '{"user_id":"test","organization_id":"org","role_id":1,"language":"es"}'

# Test completo (con LLM)
curl -X POST http://localhost:8001/api/v1/interviews/start \
  -H "Content-Type: application/json" \
  -d '{"user_id":"user-123","organization_id":"org-456","role_id":1,"language":"es"}'
```

---

## ğŸ“Š **Stack TecnolÃ³gico**

- **Framework**: FastAPI 0.115.6
- **AI SDK**: Strands Agents >=1.0.0
- **LLM Local**: Ollama + Llama 3.2
- **LLM Cloud**: OpenAI GPT-4o
- **Language**: Python 3.11+
- **Config**: Pydantic Settings
- **HTTP Client**: httpx, requests
- **Containerization**: Docker + Docker Compose

---

## ğŸ¤ **IntegraciÃ³n con otros servicios**

Este microservicio es parte de un sistema mayor:

1. **Frontend (React)**: `web-frontend-react/`
   - Consume este servicio para el chat
   - Maneja la UI y persistencia en localStorage

2. **Backend PHP**: `svc-organizations-php/`
   - Provee contexto de usuarios/organizaciones
   - PersistirÃ¡ entrevistas completadas (futuro)

3. **AnÃ¡lisis de Procesos** (Futuro):
   - ConsumirÃ¡ el endpoint `/export`
   - ExtraerÃ¡ procesos estructurados
   - GenerarÃ¡ diagramas BPMN

---

## ğŸ“ **PrÃ³ximos Pasos**

- [ ] Implementar persistencia en PostgreSQL (actualmente solo localStorage)
- [ ] Agregar anÃ¡lisis de sentimientos
- [ ] Spell checking en espaÃ±ol
- [ ] Soporte para mÃ¡s idiomas (FR, IT, DE)
- [ ] MÃ©tricas con Prometheus
- [ ] Tests automatizados (pytest)
- [ ] CI/CD pipeline

---

## ğŸ“„ **Licencia**

Este proyecto es parte de una tesis de grado.

---

## ğŸ‘¨â€ğŸ’» **Autor**

Desarrollado como parte del proyecto de tesis "Sistema de ElicitaciÃ³n de Requerimientos con IA".

---

## ğŸ†˜ **Soporte**

Si encontrÃ¡s algÃºn problema:
1. VerificÃ¡ la secciÃ³n **Troubleshooting**
2. RevisÃ¡ los logs: `docker-compose logs -f elicitation-ai`
3. ConsultÃ¡ la documentaciÃ³n interactiva: http://localhost:8001/docs
4. RevisÃ¡ las issues en el repositorio

---

## â“ **Preguntas Frecuentes (FAQ)**

### **1. Â¿Necesito instalar Ollama en mi mÃ¡quina?**

**Con Docker (recomendado):** âŒ NO
- Ollama corre dentro de un contenedor Docker
- El modelo se descarga dentro del contenedor
- Todo estÃ¡ aislado, no "ensucia" tu mÃ¡quina

**Sin Docker (setup manual):** âœ… SÃ
- NecesitÃ¡s instalar Ollama desde https://ollama.com/download
- El modelo se descarga en tu mÃ¡quina (~2GB)

---

### **2. Â¿Necesito instalar Python?**

**Con Docker:** âŒ NO
- Python y todas las dependencias estÃ¡n en el contenedor

**Sin Docker:** âœ… SÃ
- Python 3.11+
- pip
- requirements.txt

---

### **3. Â¿CuÃ¡nto espacio ocupa todo?**

**Docker:**
- Imagen de Python: ~500 MB
- Imagen de Ollama: ~500 MB
- Modelo Llama 3.2 3B: ~2 GB
- **Total: ~3 GB**

---

### **4. Â¿Puedo usar esto sin internet?**

**Con Ollama (local):** âœ… SÃ
- Una vez descargado el modelo, funciona 100% offline

**Con OpenAI:** âŒ NO
- Necesita conexiÃ³n a internet siempre

---

### **5. Â¿Es gratis?**

**Ollama:** âœ… 100% gratis, sin lÃ­mites

**OpenAI:** âŒ ~$0.01 por entrevista (requiere API key de pago)

---

### **6. Â¿QuÃ© pasa si apago la PC?**

- Los contenedores se detienen
- Al hacer `docker-compose up -d` de nuevo, todo vuelve como estaba
- **El modelo NO se borra**, queda guardado en un volumen de Docker

---

### **7. Â¿CÃ³mo borro todo?**

```bash
# Borrar contenedores y el modelo descargado
docker-compose down -v

# Borrar tambiÃ©n las imÃ¡genes (libera ~3GB)
docker rmi svc-elicitation-ai-elicitation-ai ollama/ollama
```

---

## ğŸ“ **Comandos mÃ¡s usados (Cheat Sheet)**

```bash
# ğŸš€ INICIO RÃPIDO
docker-compose up -d                                    # Levantar servicios
docker exec ollama-service ollama pull llama3.2:3b     # Descargar modelo (solo 1ra vez)
curl http://localhost:8001/api/v1/health               # Verificar que funciona

# ğŸ“Š MONITOREO
docker ps                                               # Ver contenedores corriendo
docker-compose logs -f                                  # Ver logs en vivo
docker stats                                            # Ver uso de recursos (RAM/CPU)

# ğŸ”„ GESTIÃ“N
docker-compose restart elicitation-ai                   # Reiniciar AI service
docker-compose down                                     # Detener todo
docker-compose up -d --build                            # Rebuild y reiniciar

# ğŸ” DEBUG
docker exec ollama-service ollama list                  # Ver modelos instalados
docker exec svc-elicitation-ai printenv | grep MODEL   # Ver config del modelo
docker logs svc-elicitation-ai --tail 50               # Ver Ãºltimos 50 logs

# ğŸ§ª TESTING
# PowerShell:
Invoke-RestMethod http://localhost:8001/api/v1/health
$body = @{ language = 'es'; organization_id = '1'; role_id = '1' } | ConvertTo-Json
Invoke-RestMethod -Uri http://localhost:8001/api/v1/interviews/start -Method Post -Body $body -ContentType 'application/json'

# Linux/Mac/Git Bash:
curl http://localhost:8001/api/v1/health
curl -X POST http://localhost:8001/api/v1/interviews/start -H "Content-Type: application/json" -d '{"language":"es","organization_id":"1","role_id":"1"}'
```

---

**Â¡Listo para empezar! ğŸš€**
